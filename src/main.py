from typing import List, Dict, Any
import os
import json # Added for potentially serializing complex objects for langgraph

from src.models import RetrospectiveSummary, AlertItem
from src.data_generator import generate_random_alert_item, enrich_alert_items, group_alerts_into_summaries

# --- Langgraph Integration ---
# This section will require specific details about your Langgraph setup.

# Placeholder for actual Langgraph client/API setup
# Example: from langgraph_client import LanggraphClient
# Example: LANGGRAPH_API_KEY = os.environ.get("LANGGRAPH_API_KEY")
# Example: client = LanggraphClient(api_key=LANGGRAPH_API_KEY)

def format_summary_for_langgraph(summary: RetrospectiveSummary) -> Dict[str, Any]:
    """
    Formats a RetrospectiveSummary object into a dictionary suitable for Langgraph input.
    This is a placeholder and will likely need to be adjusted based on the
    specific Langgraph graph's expected input schema.
    """
    alerts_data = []
    for item in summary.items:
        alerts_data.append({
            "id": item.id,
            "title": item.title,
            "status": item.status,
            "created_date": item.created_date.isoformat(),
            "is_noisy": item.is_noisy,
            "is_self_resolved": item.is_self_resolved,
            "alert_node_analysis": item.alert_node_analysis,
            "graph_analysis": item.graph_analysis,
        })

    return {
        "summary_id": summary.summary_id,
        "tags": sorted(list(summary.tags)), # Consistent order for tags
        "alerts": alerts_data,
        "item_count": len(summary.items),
        "generated_at": summary.generated_at.isoformat()
    }

def process_summary_with_langgraph(summary: RetrospectiveSummary) -> str:
    """
    Processes a RetrospectiveSummary through Langgraph to generate actionable insights.

    This is a **placeholder** function. You will need to replace this with
    the actual Langgraph API call and response handling.

    Args:
        summary: The RetrospectiveSummary object to process.

    Returns:
        A string containing actionable insights generated by Langgraph.
    """
    print(f"\n[Langgraph SIMULATION] Processing summary ID: {summary.summary_id} with tags: {summary.tags}")

    # 1. Format the input for Langgraph
    # This might involve converting the summary object to a JSON string or a specific dict structure.
    langgraph_input = format_summary_for_langgraph(summary)

    # Simulate sending data to Langgraph and receiving a response.
    # In a real scenario, this would be an API call.
    # Example:
    # if not LANGGRAPH_API_KEY:
    #     return "Error: LANGGRAPH_API_KEY not configured."
    # try:
    #     # response = client.invoke("your_graph_id", langgraph_input)
    #     # insights = response.get("output_field_name", "No insights generated.")
    #     insights = f"Mock insight for summary {summary.summary_id}: Based on {len(summary.items)} alerts tagged with {summary.tags}, consider investigating recurring patterns in {summary.items[0].alert_node_analysis.get('component', 'N/A')}."
    # except Exception as e:
    #     print(f"Error calling Langgraph: {e}")
    #     insights = f"Error processing summary {summary.summary_id} with Langgraph."

    # --- Actual Langgraph Integration ---
    # Ensure GEMINI_API_KEY is available
    if not os.getenv("GEMINI_API_KEY"):
        print("Error: GEMINI_API_KEY environment variable not set.")
        return "Error: GEMINI_API_KEY not configured. Please set it in a .env file or environment."

    try:
        # Import graph components from backend
        from backend.graph import workflow as base_workflow, GraphState, generate_actionable_insights_node
        from langgraph.graph import StateGraph, END
        from langgraph.checkpoint.memory import MemorySaver # Using MemorySaver for simplicity

        # We need a graph that can directly run our 'generate_actionable_insights_node'.
        # We can define a new mini-workflow for this or invoke the node if the main app does.
        # For this script, let's define a workflow that uses this node as an entry point.
        insights_workflow = StateGraph(GraphState)
        # The lambda for add_node now needs to correctly pass the input to the node function.
        # The key in invoke (e.g. "retrospective_summary_input") must match the parameter name in the lambda.
        insights_workflow.add_node("generate_actionable_insights",
                                   lambda state, retrospective_summary_input_arg: generate_actionable_insights_node(state, retrospective_summary_input_arg))
        insights_workflow.set_entry_point("generate_actionable_insights")
        insights_workflow.add_edge("generate_actionable_insights", END)

        # Compile this specific workflow
        # Each thread_id gets its own state. For one-off insights, a unique ID can be used.
        memory_saver = MemorySaver()
        insights_graph = insights_workflow.compile(checkpointer=memory_saver)

        # Prepare input for the langgraph node
        langgraph_input_payload = format_summary_for_langgraph(summary)

        # Invoke the graph
        # The input to invoke must be a dict where the key matches the parameter name in the lambda
        # used in `insights_workflow.add_node` for the entrypoint node.
        # So, {"retrospective_summary_input_arg": langgraph_input_payload}
        thread_id = f"insights-{summary.summary_id}" # Unique thread for this one-off task
        config = {"configurable": {"thread_id": thread_id}}

        print(f"\n[Langgraph] Calling 'generate_actionable_insights' for summary ID: {summary.summary_id}")
        print(f"[Langgraph] Input: {json.dumps(langgraph_input_payload, indent=2, default=str)}")

        # The key in the input dictionary for invoke should match the input argument name expected by the lambda
        # which wraps our node in the StateGraph definition.
        # In `insights_workflow.add_node("generate_actionable_insights", lambda state, retrospective_summary_input_arg: ...)`
        # the second parameter to the lambda is `retrospective_summary_input_arg`.
        # So, the invoke call should be:
        graph_output = insights_graph.invoke({"retrospective_summary_input_arg": langgraph_input_payload}, config=config)

        print(f"[Langgraph] Raw output: {graph_output}")

        if graph_output and graph_output.get("error"):
            print(f"Error from Langgraph: {graph_output['error']}")
            return f"Error generating insights: {graph_output['error']}"

        insights = graph_output.get("actionable_insights", "No insights generated or key mismatch.")

        if not insights: # If insights is empty string or None
             insights = "Langgraph returned empty insights. Check LLM interaction or prompt."

    except ImportError as e:
        print(f"Error importing Langgraph components: {e}. Make sure backend.graph is accessible and dependencies are installed.")
        return "Error: Langgraph components not found. Setup issue."
    except Exception as e:
        print(f"An unexpected error occurred during Langgraph processing: {e}")
        # import traceback
        # traceback.print_exc()
        return f"Unexpected error: {str(e)}"

    print(f"[Langgraph] Insights received: {insights}")
    return insights

# --- Main Workflow ---
def run_alert_processing_pipeline(num_alerts_to_generate: int = 20, noisy_alert_threshold: int = 2, max_items_per_summary: int = 5):
    # Load .env file for GEMINI_API_KEY
    from dotenv import load_dotenv
    load_dotenv()
    """
    Runs the full pipeline:
    1. Generates mock alerts.
    2. Enriches the alerts.
    3. Groups alerts into retrospective summaries.
    4. Processes each summary through Langgraph to get actionable insights.
    """
    print("Starting alert processing pipeline...")

    # 1. Generate mock alerts
    print(f"\nStep 1: Generating {num_alerts_to_generate} mock alerts...")
    mock_alerts = [generate_random_alert_item() for _ in range(num_alerts_to_generate)]
    print(f"Generated {len(mock_alerts)} alerts.")
    # for alert in mock_alerts[:2]:
    #     print(f"  - {alert}")

    # 2. Enrich these alerts
    print("\nStep 2: Enriching alerts...")
    enriched_alerts = enrich_alert_items(mock_alerts, noisy_threshold_count=noisy_alert_threshold)
    noisy_count = sum(1 for alert in enriched_alerts if alert.is_noisy)
    self_resolved_count = sum(1 for alert in enriched_alerts if alert.is_self_resolved)
    print(f"Enrichment complete. Noisy alerts: {noisy_count}, Self-resolved alerts: {self_resolved_count}.")
    # for alert in enriched_alerts[:2]:
    #     print(f"  - {alert}")

    # 3. Group alerts into retrospective summaries
    print("\nStep 3: Grouping alerts into retrospective summaries...")
    retrospective_summaries = group_alerts_into_summaries(enriched_alerts, max_alerts_per_summary=max_items_per_summary)
    print(f"Generated {len(retrospective_summaries)} retrospective summaries.")
    # for summary in retrospective_summaries[:2]:
    #     print(f"  - {summary}")

    # 4. Process summaries through Langgraph
    print("\nStep 4: Processing summaries through Langgraph for actionable insights...")
    if not retrospective_summaries:
        print("No summaries generated to process.")
        return

    all_insights = []
    for summary in retrospective_summaries:
        insights = process_summary_with_langgraph(summary) # This is the call to the (currently mock) Langgraph function
        all_insights.append({
            "summary_id": summary.summary_id,
            "tags": summary.tags,
            "insight": insights
        })
        print(f"Insight for Summary ID {summary.summary_id}: {insights}")

    print("\n--- Pipeline Complete ---")
    print(f"Processed {len(retrospective_summaries)} summaries and generated {len(all_insights)} insights.")
    # Optionally, save insights to a file or database here

if __name__ == "__main__":
    run_alert_processing_pipeline(num_alerts_to_generate=50, noisy_alert_threshold=3, max_items_per_summary=7)
